{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 專案介紹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- jpg file: 彩色影像，應該是來自賽車遊戲 SuperTuxKart 的環境截圖。\n",
    "- png file: 深度圖，應該表示場景中每個像素距離相機的深度資訊。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 攝影機拍攝的 2D 影像，實際上是在一個 3D 空間內捕捉畫面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](drive_data/train/cornfield_crossing_00/00000_im.jpg)\n",
    "\n",
    "![image info](drive_data/train/cornfield_crossing_00/00000_depth.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 標籤檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['track', 'frames']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "file_path = \"drive_data/train/hacienda_00/info.npz\"\n",
    "data = np.load(file_path, allow_pickle=True)\n",
    "print(data.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- path_nodes（賽道節點）\n",
    "  - 座標 (x, y, z) 是基於 3D 遊戲引擎的座標系統，用來表示賽道上的節點位置。\n",
    "  - 車輛應該沿著這些節點行駛。\n",
    "  - 第一個節點 (0.41, 0.355, 18.822) 是起點。\n",
    "  - 第二個節點 (0.41, 0.355, 24.698) 是下一個應該到達的地方。\n",
    "\n",
    "- path_distance（節點間距離）\n",
    "  - 從起點（第一個節點）到其他節點的累計距離。\n",
    "  - 計算它已經在賽道上行駛了多遠，有助於導航規劃，根據這些距離來預測前方路徑行駛速度和方向。\n",
    "  - 如果 path_distance 變大，代表車輛正在往前移動。\n",
    "  - 第一個節點的距離 = 0.0（起點）\n",
    "  - 第二個節點的距離 = 5.8759995（從第一個節點到這個節點的距離）\n",
    "\n",
    "- path_width（賽道寬度）\n",
    "  - 指定了該路徑的寬度，單位應該是米（m），車輛可以在這個範圍內移動。\n",
    "  - 告訴車輛可以在這條賽道內左右移動，不會超出界線。\n",
    "  - 如果賽道變窄（例如 path_width = 5），車輛就要小心不能偏離中心，避免撞牆。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key = path_nodes \n",
      "[[ 0.   -0.99  0.  ]\n",
      " [ 0.   -0.99 10.  ]]\n",
      "\n",
      "key = path_distance \n",
      "[ 0. 10.]\n",
      "\n",
      "key = path_width \n",
      "[8.000168]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in data['track'].item().keys():\n",
    "    value = data['track'].item()[key]\n",
    "    print(f\"key = {key} \\n{value[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- V：(View Matrix, 視圖矩陣)\n",
    "  - 將 世界座標 (world coordinates) 轉換到 相機座標系 (camera coordinates)。\n",
    "  - 4x4 齊次變換矩陣 (Homogeneous Transformation Matrix)\n",
    "     - 前三行三列 3x3 區域：旋轉矩陣，決定了相機的方向。\n",
    "     - 最後一列前三個值：相機的位置 (x, y, z)\n",
    "     -  後一行 (0,0,0,1)：是齊次座標變換的一部分，確保矩陣可以正確應用於 3D 點。\n",
    "\n",
    "- P：(Projection Matrix, 投影矩陣)\n",
    "  - 將 相機座標系的 3D 點投影到 2D 屏幕。\n",
    "  - 透視投影矩陣 (Perspective Projection Matrix)\n",
    "    - P[0,0] = 0.8938152：代表水平 (X) 方向的縮放因子。\n",
    "    - P[1,1] = 1.1917536：代表垂直 (Y) 方向的縮放因子。\n",
    "    - P[2,2] = 1.001001，P[2,3] = 1，P[3,2] = -1.001001：這些值用來進行透視投影，使物體遠小近大。\n",
    "    - P[3,3] = 0：表示這是透視投影，而非正交投影。\n",
    "\n",
    "- location：(相機位置)\n",
    "  - 相機在 3D 世界中的位置 (x, y, z)。\n",
    "- front：(相機朝向)\n",
    "  - 相機朝向正前方的一個點坐標 (x, y, z)。\n",
    "\n",
    "- velocity(速度向量)\n",
    "  - 速度 (vx, vy, vz)，表示車輛當前的速度。\n",
    "  - vx 和 vy 很小，表示車子當前幾乎靜止或移動速度很慢。\n",
    "\n",
    "- distance_down_track：(賽道上的距離)\n",
    "  - 代表 車輛在賽道上的累計距離 (沿賽道的行駛距離)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key = V\n",
      "[[ 9.9995995e-01  8.5655162e-03 -2.6082825e-03  0.0000000e+00]\n",
      " [-8.8857561e-03  9.8517323e-01 -1.7133233e-01  0.0000000e+00]\n",
      " [ 1.1020603e-03  1.7134863e-01  9.8520982e-01  0.0000000e+00]\n",
      " [ 7.2150356e-01  5.8840716e-01  4.2071357e+00  1.0000000e+00]]\n",
      "\n",
      "key = P\n",
      "[[ 0.8938152  0.         0.         0.       ]\n",
      " [ 0.         1.1917536  0.         0.       ]\n",
      " [ 0.         0.         1.0004002  1.       ]\n",
      " [ 0.         0.        -1.0004002  0.       ]]\n",
      "\n",
      "key = location\n",
      "[-0.72824645 -0.7002725  -2.2456324 ]\n",
      "\n",
      "key = front\n",
      "[-0.72835624 -0.6994907  -1.5271327 ]\n",
      "\n",
      "key = velocity\n",
      "[ 0.00284756  0.08167525 -0.00017326]\n",
      "\n",
      "key = distance_down_track\n",
      "0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in data['frames'].item().keys():\n",
    "    value = data['frames'].item()[key]\n",
    "    print(f\"key = {key}\\n{value[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實作-計算左右邊界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `road_utils.py` 根據原始導航點 (path_nodes) 計算出道路的左右邊界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center points:\n",
      " [-1.76699624e-06 -9.90008039e-01 -5.09255339e-06] \n",
      "\n",
      "Left boundary:\n",
      " [-4.00008169e+00 -9.90008039e-01 -8.99676112e-06] \n",
      "\n",
      "Right boundary:\n",
      " [ 4.00007816e+00 -9.90008039e-01 -1.18834566e-06] \n",
      "\n",
      "Center distance:\n",
      " 0.0 \n",
      "\n",
      "Track width:\n",
      " [8.00016785] \n",
      "\n",
      "Homogeneous left track:\n",
      " [-4.00008169e+00 -9.90008039e-01 -8.99676112e-06  1.00000000e+00] \n",
      "\n",
      "Homogeneous right track:\n",
      " [ 4.00007816e+00 -9.90008039e-01 -1.18834566e-06  1.00000000e+00] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from homework.datasets.road_utils import Track\n",
    "\n",
    "track = Track(**data[\"track\"].item())\n",
    "type(track)\n",
    "\n",
    "#### 屬性\n",
    "# 道路中心點\n",
    "print(\"Center points:\\n\", track.center[0], '\\n')\n",
    "# 左邊界\n",
    "print(\"Left boundary:\\n\", track.left[0], '\\n')\n",
    "# 右邊界\n",
    "print(\"Right boundary:\\n\", track.right[0], '\\n')\n",
    "# 累積距離\n",
    "print(\"Center distance:\\n\", track.center_distance[0], '\\n')\n",
    "# 道路寬度\n",
    "print(\"Track width:\\n\", track.width[0], '\\n')\n",
    "\n",
    "#### 齊次座標 (Homogeneous Coordinates)\n",
    "# 道路邊界的左法向量\n",
    "print(\"Homogeneous left track:\\n\", track.track_left[0], '\\n')\n",
    "# 道路邊界的右法向量\n",
    "print(\"Homogeneous right track:\\n\", track.track_right[0], '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實作-資料入與出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `homework.datasets.`\n",
    "\n",
    "  - 匯入路徑\n",
    "\n",
    "  - 匯出所需資料\n",
    "\n",
    "  - 包含 `road_transforms`、`road_utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys:\n",
      " dict_keys(['image', 'track_left', 'track_right', 'waypoints', 'waypoints_mask'])\n",
      "Image:\n",
      " [0.20784314 0.20784314 0.21176471 0.21176471 0.21176471 0.21176471\n",
      " 0.21176471 0.21176471 0.20392157 0.20392157 0.20392157 0.20392157\n",
      " 0.20392157 0.20392157 0.20392157 0.20392157 0.21568628 0.21568628\n",
      " 0.21568628 0.21176471 0.21176471 0.20784314 0.20784314 0.20784314\n",
      " 0.2        0.2        0.2        0.2        0.2        0.2\n",
      " 0.2        0.2        0.1764706  0.1764706  0.1764706  0.1764706\n",
      " 0.1764706  0.17254902 0.18039216 0.18039216 0.17254902 0.17254902\n",
      " 0.1764706  0.1764706  0.1764706  0.17254902 0.17254902 0.17254902\n",
      " 0.18039216 0.18039216 0.18431373 0.18431373 0.1882353  0.19215687\n",
      " 0.19607843 0.2        0.19607843 0.2        0.20784314 0.20784314\n",
      " 0.21568628 0.21960784 0.21960784 0.21568628 0.21176471 0.20784314\n",
      " 0.20784314 0.21176471 0.21176471 0.21568628 0.21568628 0.21568628\n",
      " 0.21568628 0.21176471 0.21176471 0.21568628 0.21568628 0.22352941\n",
      " 0.22745098 0.23137255 0.24705882 0.24705882 0.24705882 0.2509804\n",
      " 0.2509804  0.2509804  0.25490198 0.25490198 0.25882354 0.26666668\n",
      " 0.27450982 0.28235295 0.28627452 0.28235295 0.27450982 0.27450982\n",
      " 0.2784314  0.2784314  0.2784314  0.26666668 0.25882354 0.2509804\n",
      " 0.25490198 0.24705882 0.2509804  0.2509804  0.25490198 0.2509804\n",
      " 0.25490198 0.2509804  0.25490198 0.25490198 0.23921569 0.2784314\n",
      " 0.2509804  0.28627452 0.29803923 0.2784314  0.26666668 0.3019608\n",
      " 0.         0.09019608 0.21176471 0.24313726 0.23921569 0.23921569\n",
      " 0.23529412 0.21568628]\n",
      "Image Size:\n",
      " (3, 96, 128)\n",
      "Track Left:\n",
      " [[-3.271466   2.2457922]\n",
      " [-3.2710733  4.7457557]\n",
      " [-3.2706807  7.2457194]]\n",
      "Track Right:\n",
      " [[4.7286215 2.24457  ]\n",
      " [4.7289925 4.7445335]\n",
      " [4.7293634 7.2444973]]\n",
      "Waypoints:\n",
      " [[0.02109061 0.69049346]\n",
      " [0.08851425 2.325323  ]\n",
      " [0.1806708  4.3402467 ]]\n",
      "Waypoints Mask:\n",
      " [ True  True  True]\n"
     ]
    }
   ],
   "source": [
    "from homework.datasets.road_dataset import RoadDataset\n",
    "\n",
    "dataset_path = \"drive_data/train/hacienda_00\"\n",
    "dataset = RoadDataset(dataset_path)\n",
    "\n",
    "sample = dataset[0]\n",
    "idx = 0\n",
    "sample = dataset[idx]\n",
    "\n",
    "print(\"keys:\\n\", sample.keys())\n",
    "print(\"Image Size:\\n\", sample[\"image\"].shape)         # 圖片大小\n",
    "print(\"Track Left:\\n\", sample[\"track_left\"][:3])      # 10*2, n=10 太多了，只取前3個點示意\n",
    "print(\"Track Right:\\n\", sample[\"track_right\"][:3])    # 10*2, n=10 太多了，只取前3個點示意\n",
    "print(\"Waypoints:\\n\", sample[\"waypoints\"])            # 這三個 waypoints 都是「有效的」 (沒有填充點 (padding))\n",
    "print(\"Waypoints Mask:\\n\", sample[\"waypoints_mask\"])  # n=3，要補足到三個點，補的點是False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:\n",
      " 0.20784314\n",
      "Image:\n",
      " (128,)\n",
      "Image:\n",
      " (96, 128)\n",
      "Image:\n",
      " (3, 96, 128)\n",
      "Image:\n",
      " [0.20784314 0.21568628 0.22352941 0.23529412 0.23921569 0.24313726\n",
      " 0.24313726 0.24313726 0.24705882 0.25490198 0.26666668 0.2784314\n",
      " 0.2901961  0.29411766 0.29803923 0.30588236 0.30980393 0.32156864\n",
      " 0.32156864 0.30588236 0.29803923 0.31764707 0.34509805 0.3882353\n",
      " 0.7411765  0.8039216  0.9764706  0.8901961  0.49411765 0.45490196\n",
      " 0.40392157 0.42352942 0.4509804  0.40784314 0.39215687 0.78431374\n",
      " 0.91764706 0.8745098  0.8901961  0.94509804 0.92156863 0.89411765\n",
      " 0.87058824 0.8627451  0.84705883 0.83137256 0.8352941  0.8784314\n",
      " 0.8862745  0.8039216  0.7607843  0.7058824  0.69411767 0.7176471\n",
      " 0.6666667  0.64705884 0.61960787 0.5647059  0.5372549  0.5529412\n",
      " 0.5686275  0.5882353  0.6156863  0.60784316 0.58431375 0.5882353\n",
      " 0.6117647  0.63529414 0.62352943 0.59607846 0.5921569  0.6117647\n",
      " 0.61960787 0.6313726  0.6        0.5803922  0.6039216  0.57254905\n",
      " 0.5176471  0.5137255  0.5686275  0.58431375 0.59607846 0.5921569\n",
      " 0.5803922  0.5764706  0.5921569  0.6117647  0.6156863  0.6\n",
      " 0.5803922  0.5568628  0.54509807 0.54901963 0.56078434 0.5764706 ]\n",
      "Image:\n",
      " (96,)\n",
      "Image:\n",
      " [0.20784314 0.40784314 0.81960785]\n",
      "Image:\n",
      " (3,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Image:\\n\", sample[\"image\"][0,0,0])\n",
    "print(\"Image:\\n\", sample[\"image\"][0,0].shape)\n",
    "print(\"Image:\\n\", sample[\"image\"][0].shape)\n",
    "print(\"Image:\\n\", sample[\"image\"].shape)\n",
    "\n",
    "print(\"Image:\\n\", sample[\"image\"][0,:,0])\n",
    "print(\"Image:\\n\", sample[\"image\"][0,:,0].shape)\n",
    "\n",
    "print(\"Image:\\n\", sample[\"image\"][:,0,0])\n",
    "print(\"Image:\\n\", sample[\"image\"][:,0,0].shape)\n",
    "\n",
    "# (3,96,128)\n",
    "# 3是通道數，96是高度，128是寬度\n",
    "# 每一張圖有 96*128 = 12288 個像素點\n",
    "# 每一個像素點有3個顏色，代表RGB三個顏色"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n",
      "tensor([[[0, 1],\n",
      "         [0, 2]],\n",
      "\n",
      "        [[0, 3],\n",
      "         [0, 4]]])\n",
      "torch.Size([2, 2, 2])\n",
      "tensor([[[1, 1],\n",
      "         [1, 2]],\n",
      "\n",
      "        [[1, 3],\n",
      "         [1, 4]]])\n",
      "torch.Size([2, 8])\n",
      "tensor([[0, 1, 0, 2, 1, 1, 1, 2],\n",
      "        [0, 3, 0, 4, 1, 3, 1, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "track_left = torch.tensor([[[0,1],[0,2]],[[0,3],[0,4]]])\n",
    "print(track_left.shape)\n",
    "print(track_left)\n",
    "\n",
    "track_right = torch.tensor([[[1,1],[1,2]],[[1,3],[1,4]]])\n",
    "print(track_right.shape)\n",
    "print(track_right)\n",
    "\n",
    "track_all = torch.cat((track_left, track_right), dim=1).flatten(start_dim=1, end_dim=2)\n",
    "print(track_all.shape)\n",
    "print(track_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 環境設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 目錄結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "homework4/\n",
    "│── assets/                 # 存放示意圖、架構圖等\n",
    "│   ├── perceiver_architecture.png\n",
    "│   ├── perceiver_io.png\n",
    "│   ├── sample.png\n",
    "│\n",
    "│── grader/                 # 評測相關\n",
    "│   ├── datasets/\n",
    "│   │   ├── road_dataset.py\n",
    "│   │   ├── road_transforms.py\n",
    "│   │   ├── road_utils.py\n",
    "│   ├── supertux_utils/\n",
    "│   │   ├── evaluate.py\n",
    "│   │   ├── video_visualization.py\n",
    "│   ├── __main__.py\n",
    "│   ├── grader.py\n",
    "│   ├── metrics.py\n",
    "│   ├── tests.py\n",
    "│\n",
    "│── homework/               # 主要作業實作\n",
    "│   ├── datasets/\n",
    "│   │   ├── road_dataset.py\n",
    "│   │   ├── road_transforms.py\n",
    "│   │   ├── road_utils.py\n",
    "│   ├── supertux_utils/\n",
    "│   │   ├── evaluate.py\n",
    "│   │   ├── video_visualization.py\n",
    "│   ├── __init__.py\n",
    "│   ├── metrics.py\n",
    "│   ├── models.py           # 你的模型實作應該在這裡！\n",
    "│   ├── train_planner.py    # 訓練腳本\n",
    "│\n",
    "│── bundle.py               # 用於打包提交\n",
    "│── README.md               # 作業說明文件\n",
    "│── requirements.txt        # 依賴套件清單\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 套件限制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- matplotlib>=3.5.0\n",
    "- Pillow>=10.0.0\n",
    "- tensorboard>=2.0.0\n",
    "- termcolor==2.4.0\n",
    "- opencv-python>=4.10.0\n",
    "- tqdm==4.66.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "conda --version\n",
    "git --version\n",
    "conda env list\n",
    "python --version\n",
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# 每秒更新一次\n",
    "nvidia-smi dmon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) 視覺化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "運行 SuperTuxKart 和視覺化腳本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install PySuperTuxKartData\n",
    "pip install PySuperTuxKart --index-url=https://www.cs.utexas.edu/~bzhou/dl_class/pystk\n",
    "\n",
    "# PySuperTuxKart requires several dependencies and has only been tested on certain systems.\n",
    "# Check out https://www.cs.utexas.edu/~bzhou/dl_class/pystk/pysupertuxkart/\n",
    "# for the full list of pre-built supported python versions / OS / CPU architectures.\n",
    "\n",
    "# If this doesn't work, you can always run your model on Colab,\n",
    "# or you can trying installing from source https://github.com/philkr/pystk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "視覺化駕駛，請參閱supertux_utils模組中的以下文件：\n",
    "\n",
    "- `evaluate.py`- 關於如何使用模型的預測來驅動以及遊戲如何運作的邏輯\n",
    "\n",
    "- `visualizations.py`- matplotlib 駕駛視覺化（需要imageio安裝）\n",
    "\n",
    "然後您可以運行以下命令來查看您的模型如何驅動："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "python -m homework.supertux_utils.evaluate --model mlp_planner --track lighthouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 評分方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "python -m grader homework -v\n",
    "\n",
    "python -m grader homework -vv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Part 1a：MLP 計畫器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目標\n",
    "  - 從車道邊界資訊來預測車輛的目標軌跡 (waypoints)\n",
    "  - 使用 多層感知機 (MLP, Multi-Layer Perceptron) \n",
    "\n",
    "- 輸入輸出\n",
    "  - 一張原生圖片資料經過 `road_dataset.py` 會有 \n",
    "    - Image Size: (3, 96, 128)\n",
    "    - Track Left: 10*2: [[x,z] ... 10* ... [x,z]]\n",
    "    - Track Right: 10*2: [[x,z] ... 10* ... [x,z]]\n",
    "    - Waypoints: n*2: [[x,z] ... n ... [x,z]]\n",
    "    - Waypoints Mask: n*2: [ T/F  ... n ... T/F]\n",
    "  - 形狀過程\n",
    "    - 輸入資料\n",
    "    - 經過 `road_dataset.py` 變成 Track Left/Right 座標 [B,10,2] & [B,10,2]\n",
    "    - 兩個合併攤開(座標消失)，[B,10,2] & [B,10,2] → [B,20,2] → [B,40]\n",
    "    - \\~~中間MLP\\~~\n",
    "    - 得到 [B, n*2]，n是預測的點數量，可更改，默認為3\n",
    "    - Waypoints 變回座標 [B, n, 2]\n",
    "    - 同時擁有 Waypoints Mask 布林值，代表該目標點是真實預測點/補充點，形狀為 [B,n]\n",
    "\n",
    "- 結構\n",
    "  - 輸入層\n",
    "    - 數據壓平成1D\n",
    "    - `Flatten()`\n",
    "  - 隱藏層\n",
    "    - Linear + ReLU\n",
    "    - `torch.nn.Linear` + `ReLU`\n",
    "  - 輸出層\n",
    "    - 匹配成 [n*2] 的形狀\n",
    "\n",
    "- 損失函數\n",
    "  - 均方誤差 MSE\n",
    "  - `torch.nn.MSELoss()`\n",
    "\n",
    "- 優化器\n",
    "  - Adam\n",
    "  - `torch.optim.Adam()`\n",
    "\n",
    "- 訓練\n",
    "  - `forward()` 前向傳播 (Forward Pass)：輸入 X 通過 MLP，獲得預測值 Y_pred。\n",
    "  - `loss` 計算損失：使用 MSE 衡量 Y_pred 和 Y 的差距。\n",
    "  - `backward()` 反向傳播 (Backward Pass)\n",
    "  - `update weights` 計算梯度並更新權重。\n",
    "  - `epochs` 重複多個 Epoch，直到收斂。\n",
    "\n",
    "- 測試\n",
    "  - 使用 測試集 (test set) 來評估 MLP 的表現。\n",
    "  - 衡量 MSE 誤差\n",
    "\n",
    "- 可視化\n",
    "  - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 成功檔案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPlanner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_track: int = 10,\n",
    "        n_waypoints: int = 3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_track (int): number of points in each side of the track\n",
    "            n_waypoints (int): number of waypoints to predict\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_track = n_track\n",
    "        self.n_waypoints = n_waypoints\n",
    "\n",
    "        layers = []\n",
    "        input_size = 2*2*n_track\n",
    "        hidden_size = [64,32,32,32]\n",
    "\n",
    "        for n_out in hidden_size:\n",
    "            layers.append(torch.nn.Linear(input_size, n_out))\n",
    "            layers.append(torch.nn.BatchNorm1d(n_out))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            input_size = n_out\n",
    "\n",
    "        layers.append(torch.nn.Linear(n_out, 2*n_waypoints))\n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        track_left: torch.Tensor,\n",
    "        track_right: torch.Tensor,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predicts waypoints from the left and right boundaries of the track.\n",
    "\n",
    "        During test time, your model will be called with\n",
    "        model(track_left=..., track_right=...), so keep the function signature as is.\n",
    "\n",
    "        Args:\n",
    "            track_left (torch.Tensor): shape (b, n_track, 2)\n",
    "            track_right (torch.Tensor): shape (b, n_track, 2)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: future waypoints with shape (b, n_waypoints, 2)\n",
    "        \"\"\"\n",
    "        track_all = torch.cat((track_left, track_right), dim=1).flatten(start_dim=1, end_dim=2)\n",
    "        waypoints = self.network(track_all)\n",
    "        waypoints = waypoints.view(-1, self.n_waypoints, 2)\n",
    "        return waypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "python -m homework.train_planner\n",
    "\n",
    "python batch_train.py        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homework.train_planner import train\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.freeze_support() \n",
    "    \n",
    "    batch_size = [\n",
    "        {\"batch_size\": 128},\n",
    "    ]\n",
    "\n",
    "    lr = [\n",
    "        {\"lr\": 0.001},\n",
    "    ]\n",
    "\n",
    "    for s in batch_size:\n",
    "        for l in lr:\n",
    "            print(f\"batch_size: {s['batch_size']}, lr: {l['lr']}\")\n",
    "            train(**s, **l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from .metrics import PlannerMetric\n",
    "from .models import load_model, save_model\n",
    "from .datasets import road_dataset\n",
    "\n",
    "def train(\n",
    "    exp_dir: str = \"logs\",             # 儲存 TensorBoard 日誌和模型檔案的資料夾\n",
    "    model_name: str = \"mlp_planner\",   # 模型名稱 (mlp_planner, transformer_planner, cnn_planner)\n",
    "    num_epoch: int = 21,                # 訓練的總輪數 epoch\n",
    "    lr: float = 0.001,                   # 學習率\n",
    "    batch_size: int = 128,             # 每個 batch 的大小\n",
    "    seed: int = 2024,                  # 隨機種子\n",
    "    **kwargs,                          # 其他模型參數\n",
    "):\n",
    "\n",
    "    # 設定 GPU / CPU 運算裝置\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")        # 如果有 CUDA，則使用 GPU\n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU\")\n",
    "        device = torch.device(\"cpu\")         # 如果沒有 GPU，則使用 CPU\n",
    "\n",
    "    # 設定隨機種子以確保可重現性\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # 設定 TensorBoard 日誌儲存路徑\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    # 載入模型\n",
    "    model = load_model(model_name, **kwargs)   # 載入模型\n",
    "    model = model.to(device)                   # 將模型權重移到 GPU / CPU\n",
    "    model.train()                              # 設定模型為訓練模式\n",
    "\n",
    "    # 載入訓練和驗證資料集\n",
    "    train_data = road_dataset.load_data(\"drive_data/train\", shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "    val_data = road_dataset.load_data(\"drive_data/val\", shuffle=False)\n",
    "    # 將資料移到 GPU / CPU\n",
    "    train_data = [(i[\"track_left\"].to(device), i[\"track_right\"].to(device), i[\"waypoints\"].to(device), i[\"waypoints_mask\"].to(device)) for i in train_data]\n",
    "    val_data = [(i[\"track_left\"].to(device), i[\"track_right\"].to(device), i[\"waypoints\"].to(device), i[\"waypoints_mask\"].to(device)) for i in val_data]\n",
    "\n",
    "    # 設定損失函數和優化器\n",
    "    loss_fn_x = torch.nn.L1Loss()\n",
    "    loss_fn_y = torch.nn.SmoothL1Loss(beta=0.01) \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    # 訓練迴圈的全域變數\n",
    "    global_step = 0\n",
    "\n",
    "    # 訓練迴圈\n",
    "    for epoch in range(num_epoch):\n",
    "        # 初始化計算器\n",
    "        metric = PlannerMetric() \n",
    "        # 設定模型為訓練模式\n",
    "        model.train()\n",
    "        for track_left, track_right, waypoints, waypoints_mask in train_data:\n",
    "            # 計算模型預測\n",
    "            waypoints_pred = model(track_left, track_right)\n",
    "            # loss\n",
    "            metric.add(waypoints_pred, waypoints, waypoints_mask)\n",
    "            longitudinal_error = metric.compute()[\"longitudinal_error\"] \n",
    "            lateral_error = metric.compute()[\"lateral_error\"]\n",
    "\n",
    "            x_loss = loss_fn_x(waypoints_pred[:, :, 0] * waypoints_mask, waypoints[:, :, 0] * waypoints_mask)\n",
    "            y_loss = loss_fn_y(waypoints_pred[:, :, 1] * waypoints_mask, waypoints[:, :, 1] * waypoints_mask)\n",
    "            alpha = torch.sigmoid(torch.tensor(longitudinal_error - lateral_error))\n",
    "            loss = alpha * x_loss + (1-alpha) * y_loss\n",
    "            # 反向傳播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 更新全域步數\n",
    "            global_step += 1\n",
    "\n",
    "        # 記錄 Loss\n",
    "        writer.add_scalar(\"train/loss\", loss, global_step)\n",
    "\n",
    "        # 清除每個 epoch 的 metrics\n",
    "        metric.reset()\n",
    "        \n",
    "        # 切換到評估模式\n",
    "        # 禁用梯度計算，加快推理速度\n",
    "        model.eval()\n",
    "        for track_left, track_right, waypoints, waypoints_mask in val_data:\n",
    "            # 計算模型預測\n",
    "            with torch.inference_mode(): # 禁用梯度計算，加快推理速度\n",
    "                waypoints_pred = model(track_left, track_right)\n",
    "            # 計算驗證準確率\n",
    "            metric.add(waypoints_pred, waypoints, waypoints_mask)\n",
    "            val_long_er = metric.compute()[\"longitudinal_error\"] \n",
    "            cal_lt_er = metric.compute()[\"lateral_error\"]\n",
    "\n",
    "        # 列出誤差\n",
    "        if epoch%5 == 0:\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            print(f\"Train= Long ER: {longitudinal_error:.4f}, Lateral ER: {lateral_error:.4f}, Valid= Long ER: {val_long_er:.4f}, Lateral ER: {cal_lt_er:.4f}\")\n",
    "\n",
    "        ## Early stopping\n",
    "        if val_long_er < 0.17 and cal_lt_er < 0.55:\n",
    "            torch.save(model.state_dict(), f\"long_{val_long_er:.4f}_lateral_{cal_lt_er:.4f}_epoch{epoch}_b{batch_size}_lr{lr}.pth\")\n",
    "\n",
    "        # 清除每個 epoch 的 metrics\n",
    "        metric.reset()\n",
    "        # 將日誌寫入檔案\n",
    "        writer.flush()\n",
    "\n",
    "    # 儲存 .th 權重檔案\n",
    "    save_model(model)\n",
    "\n",
    "import argparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--num_epoch\", type=int, default=21)\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.001)\n",
    "    train(**vars(parser.parse_args()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 96, 128])\n",
      "torch.Size([256, 10, 2])\n",
      "torch.Size([256, 10, 2])\n",
      "torch.Size([256, 3, 2])\n",
      "torch.Size([256, 3])\n"
     ]
    }
   ],
   "source": [
    "for i in train_data:\n",
    "    print(i[\"image\"].shape)\n",
    "    print(i[\"track_left\"].shape)\n",
    "    print(i[\"track_right\"].shape)\n",
    "    print(i[\"waypoints\"].shape)\n",
    "    print(i[\"waypoints_mask\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Part 1b：Transformer 計畫器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用 Transformer 來預測車輛軌跡 Waypoints\n",
    "\n",
    "- 輸入來自 road_dataset.py 的輸出，每筆資料包含：\n",
    "\n",
    "  - track_left：形狀為 (B, n_track, 2)，左側邊界的 x,z 座標\n",
    "\n",
    "  - track_right：形狀為 (B, n_track, 2)，右側邊界的 x,z 座標\n",
    "\n",
    "- 預測目標 (Output):\n",
    "\n",
    "  - waypoints：預測結果形狀為 (B, n_waypoints, 2)，表示未來的導航點 (x, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原始檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPlanner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_track: int = 10,\n",
    "        n_waypoints: int = 3,\n",
    "        d_model: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_track = n_track\n",
    "        self.n_waypoints = n_waypoints\n",
    "\n",
    "        self.query_embed = nn.Embedding(n_waypoints, d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        track_left: torch.Tensor,\n",
    "        track_right: torch.Tensor,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predicts waypoints from the left and right boundaries of the track.\n",
    "\n",
    "        During test time, your model will be called with\n",
    "        model(track_left=..., track_right=...), so keep the function signature as is.\n",
    "\n",
    "        Args:\n",
    "            track_left (torch.Tensor): shape (b, n_track, 2)\n",
    "            track_right (torch.Tensor): shape (b, n_track, 2)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: future waypoints with shape (b, n_waypoints, 2)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight=\n",
      " Parameter containing:\n",
      "tensor([[-0.1868,  0.7949,  0.7277, -0.9573],\n",
      "        [-0.2949,  0.0187,  0.7739, -0.9359],\n",
      "        [-0.0500,  3.7119,  1.0982,  0.6698]], requires_grad=True)\n",
      "tensor([[ 0.7467, -2.1993,  0.5563, -0.5789],\n",
      "        [ 1.2145,  1.2570,  0.1477, -0.5473],\n",
      "        [-0.2250, -1.4292, -0.0911,  1.6844],\n",
      "        [ 1.2512,  0.1812,  0.9084,  0.0930],\n",
      "        [ 0.4955,  0.2275, -0.0736, -0.4374]])\n",
      "tensor([[-0.2250, -1.4292, -0.0911,  1.6844],\n",
      "        [ 1.2512,  0.1812,  0.9084,  0.0930],\n",
      "        [ 0.4955,  0.2275, -0.0736, -0.4374]])\n",
      "tensor([[-0.4118, -0.6343,  0.6367,  0.7271],\n",
      "        [ 0.9563,  0.1999,  1.6823, -0.8429],\n",
      "        [ 0.4455,  3.9394,  1.0246,  0.2325]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "d_model = 4\n",
    "p = 3\n",
    "query_embed = torch.nn.Embedding(p, d_model)\n",
    "print(\"weight=\\n\",query_embed.weight)\n",
    "\n",
    "x = torch.randn(5,d_model)\n",
    "print(x)\n",
    "\n",
    "x = x[-p:,...]\n",
    "print(x)\n",
    "print(x+query_embed.weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 成功檔案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPlanner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_track: int = 10,\n",
    "        n_waypoints: int = 3,\n",
    "        d_model: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_track = n_track\n",
    "        self.n_waypoints = n_waypoints\n",
    "\n",
    "        self.query_embed = nn.Embedding(n_waypoints, d_model)\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(2, d_model),\n",
    "            *[TransformerLayer(d_model, num_heads=8) for _ in range(4)],\n",
    "            #nn.Linear(d_model, 2),\n",
    "            )\n",
    "        self.output_layer = nn.Linear(d_model, 2)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        track_left: torch.Tensor,\n",
    "        track_right: torch.Tensor,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predicts waypoints from the left and right boundaries of the track.\n",
    "\n",
    "        During test time, your model will be called with\n",
    "        model(track_left=..., track_right=...), so keep the function signature as is.\n",
    "\n",
    "        Args:\n",
    "            track_left (torch.Tensor): shape (b, n_track, 2)\n",
    "            track_right (torch.Tensor): shape (b, n_track, 2)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: future waypoints with shape (b, n_waypoints, 2)\n",
    "        \"\"\"\n",
    "        track_all = torch.cat((track_left, track_right), dim=1)\n",
    "        waypoints = self.network(track_all)\n",
    "        waypoints = waypoints[..., -self.n_waypoints:, :] + self.query_embed.weight\n",
    "        waypoints = self.output_layer(waypoints)\n",
    "        return waypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "python -m homework.train_planner\n",
    "python batch_train.py        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homework.train_planner import train\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.freeze_support() \n",
    "    \n",
    "    batch_size = [\n",
    "        {\"batch_size\": 128},\n",
    "    ]\n",
    "\n",
    "    lr = [\n",
    "        {\"lr\": 0.001},\n",
    "    ]\n",
    "\n",
    "    for s in batch_size:\n",
    "        for l in lr:\n",
    "            print(f\"batch_size: {s['batch_size']}, lr: {l['lr']}\")\n",
    "            train(**s, **l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from .metrics import PlannerMetric\n",
    "from .models import load_model, save_model\n",
    "from .datasets import road_dataset\n",
    "\n",
    "def train(\n",
    "    exp_dir: str = \"logs\",\n",
    "    model_name: str = \"transformer_planner\", \n",
    "    num_epoch: int = 21, \n",
    "    lr: float = 0.001, \n",
    "    batch_size: int = 128, \n",
    "    seed: int = 2024, \n",
    "    **kwargs,\n",
    "):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")  \n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU\")\n",
    "        device = torch.device(\"cpu\") \n",
    "    torch.manual_seed(seed)\n",
    "    writer = SummaryWriter()\n",
    "    model = load_model(model_name, **kwargs)  \n",
    "    model = model.to(device) \n",
    "    model.train()\n",
    "    train_data = road_dataset.load_data(\"drive_data/train\", shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "    val_data = road_dataset.load_data(\"drive_data/val\", shuffle=False)\n",
    "    train_data = [(i[\"track_left\"].to(device), i[\"track_right\"].to(device), i[\"waypoints\"].to(device), i[\"waypoints_mask\"].to(device)) for i in train_data]\n",
    "    val_data = [(i[\"track_left\"].to(device), i[\"track_right\"].to(device), i[\"waypoints\"].to(device), i[\"waypoints_mask\"].to(device)) for i in val_data]\n",
    "    loss_fn_x = torch.nn.SmoothL1Loss(beta=0.0001)\n",
    "    loss_fn_y = torch.nn.SmoothL1Loss(beta=0.01) \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        metric = PlannerMetric() \n",
    "        model.train()\n",
    "        for track_left, track_right, waypoints, waypoints_mask in train_data:\n",
    "            waypoints_pred = model(track_left, track_right)\n",
    "            metric.add(waypoints_pred, waypoints, waypoints_mask)\n",
    "            longitudinal_error = metric.compute()[\"longitudinal_error\"] \n",
    "            lateral_error = metric.compute()[\"lateral_error\"]\n",
    "            x_loss = loss_fn_x(waypoints_pred[:, :, 0] * waypoints_mask, waypoints[:, :, 0] * waypoints_mask)\n",
    "            y_loss = loss_fn_y(waypoints_pred[:, :, 1] * waypoints_mask, waypoints[:, :, 1] * waypoints_mask)\n",
    "            alpha = torch.sigmoid(torch.tensor(longitudinal_error - lateral_error))\n",
    "            loss = alpha * x_loss + (1-alpha) * y_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "        writer.add_scalar(\"train/loss\", loss, global_step)\n",
    "        metric.reset()\n",
    "        model.eval()\n",
    "        for track_left, track_right, waypoints, waypoints_mask in val_data:\n",
    "            with torch.inference_mode():\n",
    "                waypoints_pred = model(track_left, track_right)\n",
    "            metric.add(waypoints_pred, waypoints, waypoints_mask)\n",
    "            val_long_er = metric.compute()[\"longitudinal_error\"] \n",
    "            cal_lt_er = metric.compute()[\"lateral_error\"]\n",
    "        if epoch%5 == 0:\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            print(f\"Train= Long ER: {longitudinal_error:.4f}, Lateral ER: {lateral_error:.4f}, Valid= Long ER: {val_long_er:.4f}, Lateral ER: {cal_lt_er:.4f}\")\n",
    "        if val_long_er < 0.22 and cal_lt_er < 0.52:\n",
    "            torch.save(model.state_dict(), f\"long_{val_long_er:.4f}_lateral_{cal_lt_er:.4f}_epoch{epoch}_b{batch_size}_lr{lr}.pth\")\n",
    "        metric.reset()\n",
    "        writer.flush()\n",
    "    save_model(model)\n",
    "import argparse\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--num_epoch\", type=int, default=21)\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    train(**vars(parser.parse_args()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練內容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Part 2：CNN 計畫器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用 CNN 直接從影像預測車道邊界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原始檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "class CNNPlanner(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_waypoints: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_waypoints = n_waypoints\n",
    "\n",
    "        self.register_buffer(\"input_mean\", torch.as_tensor(INPUT_MEAN), persistent=False)\n",
    "        self.register_buffer(\"input_std\", torch.as_tensor(INPUT_STD), persistent=False)\n",
    "\n",
    "    def forward(self, image: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (torch.FloatTensor): shape (b, 3, h, w) and vals in [0, 1]\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: future waypoints with shape (b, n, 2)\n",
    "        \"\"\"\n",
    "        x = image\n",
    "        x = (x - self.input_mean[None, :, None, None]) / self.input_std[None, :, None, None]\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2, 4])\n",
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.]],\n",
      "\n",
      "         [[ 8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15.]],\n",
      "\n",
      "         [[16., 17., 18., 19.],\n",
      "          [20., 21., 22., 23.]]],\n",
      "\n",
      "\n",
      "        [[[24., 25., 26., 27.],\n",
      "          [28., 29., 30., 31.]],\n",
      "\n",
      "         [[32., 33., 34., 35.],\n",
      "          [36., 37., 38., 39.]],\n",
      "\n",
      "         [[40., 41., 42., 43.],\n",
      "          [44., 45., 46., 47.]]]])\n"
     ]
    }
   ],
   "source": [
    "image = torch.arange(2*3*2*4, dtype=torch.float).reshape(2,3,2,4)\n",
    "print(image.shape)\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 3.5000]],\n",
      "\n",
      "         [[11.5000]],\n",
      "\n",
      "         [[19.5000]]],\n",
      "\n",
      "\n",
      "        [[[27.5000]],\n",
      "\n",
      "         [[35.5000]],\n",
      "\n",
      "         [[43.5000]]]])\n",
      "tensor([[ 3.5000, 11.5000, 19.5000],\n",
      "        [27.5000, 35.5000, 43.5000]])\n"
     ]
    }
   ],
   "source": [
    "pool_1 = nn.AdaptiveAvgPool2d((1,1))\n",
    "image_pool_1 = pool_1(image)\n",
    "print(image_pool_1)\n",
    "\n",
    "image_flatten = nn.Flatten()(image_pool_1)\n",
    "print(image_flatten)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 成功檔案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "class CNNPlanner(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_waypoints: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_waypoints = n_waypoints\n",
    "\n",
    "        self.register_buffer(\"input_mean\", torch.as_tensor(INPUT_MEAN), persistent=False)\n",
    "        self.register_buffer(\"input_std\", torch.as_tensor(INPUT_STD), persistent=False)\n",
    "        \n",
    "        c1 = 128\n",
    "        First_layer = [\n",
    "            nn.Conv2d(3, c1, kernel_size=7, stride=4, padding=3),\n",
    "            nn.BatchNorm2d(c1),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "\n",
    "        c2 = 64\n",
    "        times = 3\n",
    "        middle_layers = []\n",
    "        for _ in range(times):\n",
    "            middle_layers.append(nn.Conv2d(c1, c2, kernel_size=3, stride=2, padding=1))\n",
    "            middle_layers.append(nn.BatchNorm2d(c2))\n",
    "            middle_layers.append(nn.ReLU())\n",
    "            c1 = c2\n",
    "\n",
    "        out_layer = [\n",
    "            # 輸出 (B, waypoints*2)\n",
    "            # 從 3D tensor 轉換到 2D tensor\n",
    "            # 1. 平均池化: 提取寬度和高度的特徵值，減少空間資訊\n",
    "            # 把 (B, 8, H', W') 轉換成 (B, 8=8個色彩通道, 每個通道保留 1 個平均特徵值) \n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            # 2. 展平: 把 3D 轉換成 2D\n",
    "            # 原本是 (B, 8, 1, 1) 轉換成 (B, 8)\n",
    "            nn.Flatten(),\n",
    "            # 3. 輸出成預期的形狀 (B, n_waypoints*2)\n",
    "            # 從 8 個特徵值轉換成 n_waypoints*2 個特徵值 (x,z)座標\n",
    "            nn.Linear(c2, n_waypoints * 2),\n",
    "        ]\n",
    "\n",
    "        self.network = nn.Sequential(*(First_layer + middle_layers + out_layer))\n",
    "\n",
    "    def forward(self, image: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (torch.FloatTensor): shape (b, 3, h, w) and vals in [0, 1]\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: future waypoints with shape (b, n, 2)\n",
    "        \"\"\"\n",
    "        x = image # x.shape = (b, 3, h=96, w=128)\n",
    "        # 標準化\n",
    "        x = (x - self.input_mean[None, :, None, None]) / self.input_std[None, :, None, None]\n",
    "        x = self.network(x)\n",
    "        \n",
    "        return x.view(-1, self.n_waypoints, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "python -m homework.train_planner\n",
    "\n",
    "python batch_train.py        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homework.train_planner import train\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.freeze_support() \n",
    "    \n",
    "    batch_size = [\n",
    "        {\"batch_size\": 128},\n",
    "    ]\n",
    "\n",
    "    lr = [\n",
    "        {\"lr\": 0.001},\n",
    "    ]\n",
    "\n",
    "    for s in batch_size:\n",
    "        for l in lr:\n",
    "            print(f\"batch_size: {s['batch_size']}, lr: {l['lr']}\")\n",
    "            train(**s, **l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 提交作業"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "python bundle.py homework pc29368\n",
    "python -m grader pc29368.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 了解各部分程式碼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "HOMEWORK_DIR = Path(__file__).resolve().parent\n",
    "INPUT_MEAN = [0.2788, 0.2657, 0.2629]\n",
    "INPUT_STD = [0.2064, 0.1944, 0.2252]\n",
    "\n",
    "class MLPPlanner(nn.Module):raise NotImplementedError\n",
    "class TransformerPlanner(nn.Module):raise NotImplementedError\n",
    "class CNNPlanner(torch.nn.Module):raise NotImplementedError\n",
    "\n",
    "MODEL_FACTORY = {\n",
    "    \"mlp_planner\": MLPPlanner,\n",
    "    \"transformer_planner\": TransformerPlanner,\n",
    "    \"cnn_planner\": CNNPlanner,\n",
    "}\n",
    "\n",
    "# 依據 model_name 建立對應的模型\n",
    "def load_model(\n",
    "    model_name: str,\n",
    "    with_weights: bool = False,\n",
    "    **model_kwargs,\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Called by the grader to load a pre-trained model by name\n",
    "    \"\"\"\n",
    "    m = MODEL_FACTORY[model_name](**model_kwargs)\n",
    "\n",
    "    if with_weights:\n",
    "        model_path = HOMEWORK_DIR / f\"{model_name}.th\"\n",
    "        assert model_path.exists(), f\"{model_path.name} not found\"\n",
    "\n",
    "        try:\n",
    "            m.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "        except RuntimeError as e:\n",
    "            raise AssertionError(\n",
    "                f\"Failed to load {model_path.name}, make sure the default model arguments are set correctly\"\n",
    "            ) from e\n",
    "\n",
    "    # limit model sizes since they will be zipped and submitted\n",
    "    model_size_mb = calculate_model_size_mb(m)\n",
    "\n",
    "    if model_size_mb > 20:\n",
    "        raise AssertionError(f\"{model_name} is too large: {model_size_mb:.2f} MB\")\n",
    "\n",
    "    return m\n",
    "\n",
    "# 將訓練好的模型儲存成 .th 權重檔案。\n",
    "def save_model(model: torch.nn.Module) -> str:\n",
    "    \"\"\"\n",
    "    Use this function to save your model in train.py\n",
    "    \"\"\"\n",
    "    model_name = None\n",
    "\n",
    "    for n, m in MODEL_FACTORY.items():\n",
    "        if type(model) is m:\n",
    "            model_name = n\n",
    "\n",
    "    if model_name is None:\n",
    "        raise ValueError(f\"Model type '{str(type(model))}' not supported\")\n",
    "\n",
    "    output_path = HOMEWORK_DIR / f\"{model_name}.th\"\n",
    "    torch.save(model.state_dict(), output_path)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# 計算模型大小\n",
    "def calculate_model_size_mb(model: torch.nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Naive way to estimate model size\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### road_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 功能：\n",
    "\n",
    "  - 定義 RoadDataset 類別\n",
    "\n",
    "  - 讀取數據，包括影像、深度圖、.npz 文件\n",
    "\n",
    "  - 返回包含 影像、waypoints、邊界線 等資訊的 sample。\n",
    "\n",
    "- 主要步驟：\n",
    "\n",
    "  - 讀取 影像 (.jpg) 和深度 (.png)。\n",
    "\n",
    "  - 從 .npz 檔案讀取數據，並整理成 dict 格式。\n",
    "\n",
    "  - 透過 dataset[idx] 取出第 idx 幀的數據樣本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "\n",
    "from . import road_transforms\n",
    "from .road_utils import Track\n",
    "\n",
    "# 讀取賽車軌跡數據\n",
    "# 賽車軌跡數據包含了賽道的左邊界和右邊界的點座標\n",
    "# 以及賽道的中心線和車輛的當前位置等信息\n",
    "class RoadDataset(Dataset):\n",
    "    \"\"\"\n",
    "    SuperTux dataset for road detection\n",
    "    \"\"\"\n",
    "\n",
    "    # 載入數據\n",
    "    def __init__(\n",
    "        self,\n",
    "        episode_path: str,\n",
    "        transform_pipeline: str = \"default\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.episode_path = Path(episode_path)\n",
    "\n",
    "        # 載入賽道數據\n",
    "        info = np.load(self.episode_path / \"info.npz\", allow_pickle=True)\n",
    "\n",
    "        # 軌跡數據\n",
    "        self.track = Track(**info[\"track\"].item())\n",
    "        # 影像數據與其他軌跡資訊\n",
    "        self.frames: dict[str, np.ndarray] = {k: np.stack(v) for k, v in info[\"frames\"].item().items()}\n",
    "        # 決定數據如何處理\n",
    "        self.transform = self.get_transform(transform_pipeline)\n",
    "\n",
    "    # 決定 如何處理數據，支援三種模式\n",
    "    def get_transform(self, transform_pipeline: str):\n",
    "        \"\"\"\n",
    "        Creates a pipeline for processing data.\n",
    "\n",
    "        Feel free to add your own pipelines (e.g. for data augmentation).\n",
    "        Note that the grader will choose one of the predefined pipelines,\n",
    "        so be careful if you modify the existing ones.\n",
    "        \"\"\"\n",
    "        xform = None\n",
    "\n",
    "        # 1. (標準處理)\n",
    "        if transform_pipeline == \"default\":\n",
    "            # image, track_left, track_right, waypoints, waypoints_mask\n",
    "            xform = road_transforms.Compose(\n",
    "                [\n",
    "                    road_transforms.ImageLoader(self.episode_path),\n",
    "                    road_transforms.EgoTrackProcessor(self.track),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # 2.  (只使用軌跡，不使用影像)\n",
    "        elif transform_pipeline == \"state_only\":\n",
    "            # track_left, track_right, waypoints, waypoints_mask\n",
    "            xform = road_transforms.EgoTrackProcessor(self.track)\n",
    "        \n",
    "        # 3. (數據增強，尚未實作)\n",
    "        elif transform_pipeline == \"aug\":\n",
    "            # add your custom augmentations here\n",
    "            pass\n",
    "\n",
    "        if xform is None:\n",
    "            raise ValueError(f\"Invalid transform {transform_pipeline} specified!\")\n",
    "\n",
    "        return xform\n",
    "\n",
    "    # 資料集大小\n",
    "    def __len__(self):\n",
    "        return len(self.frames[\"location\"])\n",
    "\n",
    "    # 返回模型的輸入數據\n",
    "    def __getitem__(self, idx: int):\n",
    "        # 建立 sample 字典\n",
    "        sample = {\"_idx\": idx, \"_frames\": self.frames}\n",
    "        # 載入影像、處理軌跡數據\n",
    "        sample = self.transform(sample)\n",
    "\n",
    "        # 移除 _idx 和 _frames\n",
    "        # _idx 只是索引，不需要當成模型輸入。\n",
    "        # _frames 是原始影像數據，經過處理後不需要了。\n",
    "        # remove private keys\n",
    "        for key in list(sample.keys()):\n",
    "            if key.startswith(\"_\"):\n",
    "                sample.pop(key)\n",
    "\n",
    "        # 輸出：包含 影像、導航點、道路邊界 等資訊的字典。\n",
    "        return sample\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    dataset_path: str,\n",
    "    transform_pipeline: str = \"default\",\n",
    "    return_dataloader: bool = True,\n",
    "    num_workers: int = 2,\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = False,\n",
    ") -> DataLoader | Dataset:\n",
    "    \"\"\"\n",
    "    Constructs the dataset/dataloader.\n",
    "    The specified transform_pipeline must be implemented in the RoadDataset class.\n",
    "\n",
    "    Args:\n",
    "        transform_pipeline (str): 'default', 'aug', or other custom transformation pipelines\n",
    "        return_dataloader (bool): returns either DataLoader or Dataset\n",
    "        num_workers (int): data workers, set to 0 for VSCode debugging\n",
    "        batch_size (int): batch size\n",
    "        shuffle (bool): should be true for train and false for val\n",
    "\n",
    "    Returns:\n",
    "        DataLoader or Dataset\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    scenes = [x for x in dataset_path.iterdir() if x.is_dir()]\n",
    "\n",
    "    # can pass in a single scene like \"road_data/val/cornfield_crossing_04\"\n",
    "    if not scenes and dataset_path.is_dir():\n",
    "        scenes = [dataset_path]\n",
    "\n",
    "    datasets = []\n",
    "    for episode_path in sorted(scenes):\n",
    "        datasets.append(RoadDataset(episode_path, transform_pipeline=transform_pipeline))\n",
    "    dataset = ConcatDataset(datasets)\n",
    "\n",
    "    print(f\"Loaded {len(dataset)} samples from {len(datasets)} episodes\")\n",
    "\n",
    "    if not return_dataloader:\n",
    "        return dataset\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        num_workers=num_workers,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### road_transforms.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 功能：\n",
    "  - 負責 數據預處理與增強 (Data Transformations)，在讀取數據後進行處理。\n",
    "\n",
    "- 類別：\n",
    "\n",
    "  - `ImageLoader`：讀取影像並正規化 (/ 255.0)。\n",
    "\n",
    "  - `DepthLoader`：讀取深度圖並正規化 (/ 65535.0)。\n",
    "\n",
    "  - `TrackProcessor`：計算並轉換道路邊界 (track_left & track_right) 到影像座標。\n",
    "\n",
    "  - `RandomHorizontalFlip`：隨機水平翻轉影像與對應的 track。\n",
    "\n",
    "- 從 `sample` 這個字典裡提取關鍵數據\n",
    "  \n",
    "  - `location:` 目前車輛的位置。\n",
    "  \n",
    "  - `front`: 車輛的前進方向。\n",
    "  \n",
    "  - `distance_down_track`: 沿著賽道的距離。\n",
    "  \n",
    "  - `waypoints`: 從 frames[\"location\"] 提取未來的位置，作為目標點（導航點）。\n",
    "\n",
    "- 從 `from_frame()` 計算\n",
    "\n",
    "  - `track_left` 和 `track_right`\n",
    "\n",
    "    - (n_track, 2) 浮點數，左右車道邊界點\n",
    "\n",
    "    - 計算 賽道的左右邊界  (呼叫 `Track.get_boundaries()`)\n",
    "\n",
    "    - 轉換到車輛自身的座標系 (`create_pose_matrix()`) 左右邊界乘上這個矩陣，就會轉換到車輛的視角。\n",
    "\n",
    "  - `waypoints`\n",
    "\n",
    "    - (n_waypoints, 2) 浮點數，目標航點\n",
    "\n",
    "    - 來自 frames[\"location\"]，代表未來的導航點。\n",
    "\n",
    "    - 轉換到車輛自身的座標系，然後轉成 2D（只保留 x 和 z）。\n",
    "\n",
    "  - `waypoints_mask`\n",
    "\n",
    "    - (n_waypoints,) bool mask 表示「乾淨」的航路點。\n",
    "\n",
    "    - 如果 waypoints 數量少於 n_waypoints (導航點的數量)，則補足缺少的點，確保 waypoints 長度固定。\n",
    "\n",
    "    - 產生一個 mask，標記哪些是「真正的 waypoints」，哪些是「填充的無效點」。\n",
    "\n",
    "    - 後續計算時，就可以忽略掉補上的無效點，避免影響模型的學習效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file is provided as-is and does not require modification.\n",
    "If you want to add custom data augmentation during training, feel free to extend this file.\n",
    "\n",
    "Design pattern of the transforms:\n",
    "1. Take in dictionary of sample data\n",
    "2. Look for specific inputs in the sample\n",
    "3. Process the inputs\n",
    "4. Add new data to the sample\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tv_transforms\n",
    "\n",
    "from .road_utils import Track, homogeneous\n",
    "\n",
    "# 將 3D 座標點投影到 2D 影像平面。\n",
    "def project(points, view, proj, h, w):\n",
    "    points_uv_raw = points @ view @ proj\n",
    "    points_uv = points_uv_raw / points_uv_raw[:, -1:]\n",
    "\n",
    "    # convert from uv to pixel coordinates, [0, W] and [0, H]\n",
    "    points_img = points_uv[:, :2]\n",
    "    points_img[:, 0] = (points_img[:, 0] + 1) * w / 2\n",
    "    points_img[:, 1] = (1 - points_img[:, 1]) * h / 2\n",
    "\n",
    "    mask = (\n",
    "        (points_uv_raw[:, -1] > 1)  # must be in front of camera\n",
    "        & (points_uv_raw[:, -1] < 15)  # don't render too far\n",
    "        & (points_img[:, 0] >= 0)  # projected in valid img width\n",
    "        & (points_img[:, 0] < w)\n",
    "        & (points_img[:, 1] >= 0)  # projected in valid img height\n",
    "        & (points_img[:, 1] < h)\n",
    "    )\n",
    "\n",
    "    return points_img[mask], mask\n",
    "\n",
    "# 將邊界線繪製到影像上。\n",
    "def rasterize_lines(\n",
    "    points: np.ndarray,\n",
    "    canvas: np.ndarray,\n",
    "    color: int,\n",
    "    thickness: int = 4,\n",
    "):\n",
    "    for i in range(len(points) - 1):\n",
    "        start = points[i].astype(int)\n",
    "        end = points[i + 1].astype(int)\n",
    "\n",
    "        cv2.line(canvas, tuple(start), tuple(end), color, thickness)\n",
    "\n",
    "# 補足或截斷點序列到指定長度。\n",
    "def pad(points: np.ndarray, max_length: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Pads/truncates the points to a set length\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): sequence of points with shape (n, d)\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: padded points (max_length, d) and mask (max_length,)\n",
    "    \"\"\"\n",
    "    truncated_points = points[:max_length]\n",
    "\n",
    "    # create a mask denoting which points are valid\n",
    "    mask = np.ones(max_length, dtype=bool)\n",
    "    mask[len(truncated_points) :] = False\n",
    "\n",
    "    required_padding = max_length - len(truncated_points)\n",
    "\n",
    "    if required_padding > 0:\n",
    "        # pad with the last element\n",
    "        if len(truncated_points) == 0:\n",
    "            padding = np.zeros((required_padding, points.shape[1]), dtype=np.float32)\n",
    "        else:\n",
    "            padding = np.repeat(truncated_points[-1:], required_padding, axis=0)\n",
    "        padded_points = np.concatenate([truncated_points, padding])\n",
    "    else:\n",
    "        padded_points = truncated_points\n",
    "\n",
    "    return padded_points, mask\n",
    "\n",
    "# 矩陣轉換\n",
    "def create_pose_matrix(\n",
    "    location: np.ndarray,\n",
    "    front: np.ndarray,\n",
    "    up: np.ndarray = [0, 1, 0],\n",
    "    eps: float = 1e-5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        location: cart position\n",
    "        front: Point the camera is looking at\n",
    "        up: up vector, default is Y-up [0, 1, 0]\n",
    "\n",
    "    Returns:\n",
    "        4x4 matrix\n",
    "    \"\"\"\n",
    "    forward = front - location\n",
    "    forward = forward / (np.linalg.norm(forward) + eps)\n",
    "\n",
    "    # calculate right vector (x-axis)\n",
    "    right = np.cross(forward, up)\n",
    "    right = right / (np.linalg.norm(right) + eps)\n",
    "\n",
    "    # recalculate up vector (y-axis) to ensure orthogonality\n",
    "    up = np.cross(right, forward)\n",
    "\n",
    "    # create matrix representations and compose\n",
    "    R = np.eye(4)\n",
    "    R[:3, :3] = np.vstack((-right, up, forward))\n",
    "    T = np.eye(4)\n",
    "    T[:3, 3] = -location\n",
    "    pose_matrix = R @ T\n",
    "\n",
    "    return pose_matrix\n",
    "\n",
    "# transforms 是 torchvision.transforms 的一個子類別\n",
    "# 它可以將多個轉換組合在一起，並且可以在一次呼叫中應用所有轉換。\n",
    "# 這樣可以簡化數據預處理的過程，並且使代碼更具可讀性。\n",
    "# 這個類別的主要作用是將多個轉換組合在一起，並且可以在一次呼叫中應用所有轉換。\n",
    "class Compose(tv_transforms.Compose):\n",
    "    def __call__(self, sample: dict):\n",
    "        for t in self.transforms:\n",
    "            sample = t(sample)\n",
    "        return sample\n",
    "\n",
    "# 讀取影像並正規化\n",
    "# 這個類別的主要作用是讀取影像並將其轉換為 numpy 陣列，然後將其正規化到 [0, 1] 的範圍內。\n",
    "# 這樣可以使影像數據更適合用於訓練深度學習模型。\n",
    "class ImageLoader:\n",
    "    def __init__(self, episode_path: str):\n",
    "        self.episode_path = Path(episode_path)\n",
    "\n",
    "    def __call__(self, sample: dict):\n",
    "        image_path = self.episode_path / f\"{sample['_idx']:05d}_im.jpg\"\n",
    "        image = np.uint8(Image.open(image_path)) / 255.0\n",
    "        image = image.transpose(2, 0, 1)\n",
    "\n",
    "        sample[\"image\"] = image.astype(np.float32)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# 讀取深度圖並正規化\n",
    "# 這個類別的主要作用是讀取深度圖並將其轉換為 numpy 陣列，然後將其正規化到 [0, 1] 的範圍內。\n",
    "# 這樣可以使深度圖數據更適合用於訓練深度學習模型。\n",
    "class DepthLoader(ImageLoader):\n",
    "    def __call__(self, sample: dict):\n",
    "        depth_path = self.episode_path / f\"{sample['_idx']:05d}_depth.png\"\n",
    "        depth = np.uint16(Image.open(depth_path)) / 65535.0\n",
    "\n",
    "        sample[\"depth\"] = depth.astype(np.float32)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# 隨機水平翻轉影像與對應的 track。\n",
    "# 這個類別的主要作用是隨機翻轉影像和對應的 track，以增加數據的多樣性。\n",
    "# 這樣可以使模型更具魯棒性，並且能夠更好地適應不同的場景和條件。\n",
    "class RandomHorizontalFlip(tv_transforms.RandomHorizontalFlip):\n",
    "    def __call__(self, sample: dict):\n",
    "        if np.random.rand() < self.p:\n",
    "            sample[\"image\"] = np.flip(sample[\"image\"], axis=2)\n",
    "            sample[\"track\"] = np.flip(sample[\"track\"], axis=1)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# 計算並轉換道路邊界 (track_left & track_right) 到影像座標。\n",
    "class TrackProcessor:\n",
    "    \"\"\"\n",
    "    Provides segmentation labels for left and right track\n",
    "    \"\"\"\n",
    "    def __init__(self, track: Track):\n",
    "        self.track = track\n",
    "\n",
    "    def __call__(self, sample: dict):\n",
    "        idx = sample[\"_idx\"]\n",
    "        frames = sample[\"_frames\"]\n",
    "        image = sample[\"image\"]\n",
    "        distance_down_track = frames[\"distance_down_track\"][idx]\n",
    "        proj = frames[\"P\"][idx].copy()\n",
    "        view = frames[\"V\"][idx].copy()\n",
    "        view[-1, :3] += -1.0 * view[1, :3]\n",
    "\n",
    "        track_left, track_right = self.track.get_boundaries(distance_down_track)\n",
    "\n",
    "        # project to image plane\n",
    "        h, w = image.shape[1:]\n",
    "        track_left, _ = project(track_left, view, proj, h, w)\n",
    "        track_right, _ = project(track_right, view, proj, h, w)\n",
    "\n",
    "        # draw line segments onto a blank canvas\n",
    "        track = np.zeros((h, w), dtype=np.uint8)\n",
    "        rasterize_lines(track_left, track, color=1)\n",
    "        rasterize_lines(track_right, track, color=2)\n",
    "\n",
    "        sample[\"track\"] = track.astype(np.int64)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# 將賽道邊界轉換到車輛座標系 (ego coordinate system)。\n",
    "# 這個轉換是為了讓模型能夠學習到車輛在賽道上的位置和方向。\n",
    "# 這樣模型就能夠預測車輛在賽道上的行駛路徑。\n",
    "class EgoTrackProcessor:\n",
    "    \"\"\"\n",
    "    Provides round boundary point labels and target waypoints\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        track: Track,\n",
    "        n_track: int = 10,\n",
    "        n_waypoints: int = 3,\n",
    "        skip: int = 1,\n",
    "    ):\n",
    "        self.track = track\n",
    "        self.n_track = n_track\n",
    "        self.n_waypoints = n_waypoints\n",
    "        self.skip = skip\n",
    "\n",
    "    def __call__(self, sample: dict):\n",
    "        frames = sample[\"_frames\"]\n",
    "        idx = sample[\"_idx\"]\n",
    "\n",
    "        front = frames[\"front\"][idx]\n",
    "        location = frames[\"location\"][idx]\n",
    "        distance_down_track = frames[\"distance_down_track\"][idx]\n",
    "\n",
    "        # use future location as target waypoints\n",
    "        waypoints = frames[\"location\"][idx : idx + (self.n_waypoints + 1) * self.skip : self.skip][1:]\n",
    "        waypoints = homogeneous(waypoints)\n",
    "\n",
    "        sample_info = self.from_frame(location, front, distance_down_track, waypoints)\n",
    "        sample.update(sample_info)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def from_frame(\n",
    "        self,\n",
    "        location: np.ndarray,\n",
    "        front: np.ndarray,\n",
    "        distance_down_track: float,\n",
    "        waypoints: np.ndarray | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if waypoints is None:\n",
    "            waypoints = np.zeros((1, 4), dtype=np.float32)\n",
    "\n",
    "        world2ego = create_pose_matrix(location, front)\n",
    "        track_left, track_right = self.track.get_boundaries(\n",
    "            distance_down_track,\n",
    "            n_points=self.n_track,\n",
    "        )\n",
    "\n",
    "        # convert to frame of kart (ego)\n",
    "        track_left = track_left @ world2ego.T\n",
    "        track_right = track_right @ world2ego.T\n",
    "        waypoints = waypoints @ world2ego.T\n",
    "\n",
    "        # project to bird's eye view (bev)\n",
    "        track_left = track_left[:, [0, 2]]\n",
    "        track_right = track_right[:, [0, 2]]\n",
    "        waypoints = waypoints[:, [0, 2]]\n",
    "\n",
    "        # make sure points are expected size\n",
    "        track_left, _ = pad(track_left, self.n_track)\n",
    "        track_right, _ = pad(track_right, self.n_track)\n",
    "        waypoints, waypoints_mask = pad(waypoints, self.n_waypoints)\n",
    "\n",
    "        return {\n",
    "            \"track_left\": track_left.astype(np.float32),\n",
    "            \"track_right\": track_right.astype(np.float32),\n",
    "            \"waypoints\": waypoints.astype(np.float32),\n",
    "            \"waypoints_mask\": waypoints_mask,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### road_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 根據原始導航點 (path_nodes) 計算出道路的左右邊界。\n",
    "\n",
    "  - 計算道路中心\n",
    "    - `path_nodes[:, 0]` 代表道路的中心點。\n",
    "\n",
    "  - 計算道路法線 (n)\n",
    "    - 法線是 垂直於道路方向的向量，用來決定道路左右邊界的位置。\n",
    "    - 計算每個導航點與下一個導航點的方向向量。\n",
    "    - 假設 track.left 是 (x, y, z)，那麼 homogeneous(track.left) 會變成 (x, y, z, 1)\n",
    "    - 這樣的表示方法有助於矩陣變換，例如透視投影。\n",
    "\n",
    "  - 計算左右邊界\n",
    "    - `left` = 中心點 加上 法線方向 * (道路寬度的一半)\n",
    "    - `right` = 中心點 減去 法線方向 * (道路寬度的一半)\n",
    "\n",
    "  - 讓道路變成循環\n",
    "    - 將道路數據翻倍，確保導航點在某些計算時不會超出邊界 (環形賽道處理)。\n",
    "\n",
    "  - 讓導航點變得更均勻\n",
    "    - 確保導航點之間的距離相等，方便後續模型學習。\n",
    "\n",
    "- 屬性\n",
    "  - `center`：道路的中心點 (n, 3)\n",
    "\n",
    "  - `left`：道路的左邊界 (n, 3)\n",
    "\n",
    "  - `right`：道路的右邊界 (n, 3)\n",
    "\n",
    "  - `center_distance`：累積距離\n",
    "\n",
    "  - `width`：道路寬度\n",
    "\n",
    "  - `track_left`：齊次坐標版本\n",
    "\n",
    "  - `track_right`：齊次坐標版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cached_property\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 將點轉換為齊次座標。\n",
    "# 齊次座標是將三維空間中的點轉換為四維空間中的點的一種方法。\n",
    "def homogeneous(points: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        points (np.ndarray): points with shape (n, d)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: homogeneous (n, d+1)\n",
    "    \"\"\"\n",
    "    return np.concatenate([points, np.ones((len(points), 1))], axis=1)\n",
    "\n",
    "# 將點序列進行插值，使得每個點之間的距離相等。\n",
    "def interpolate_smooth(\n",
    "    points: np.ndarray,\n",
    "    fixed_distance: float | None = None,\n",
    "    fixed_number: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        points (np.ndarray): points with shape (n, d).\n",
    "        fixed_distance (float): fixed distance between points.\n",
    "        fixed_number (int): fixed number of points.\n",
    "    \"\"\"\n",
    "    if fixed_distance is None and fixed_number is None:\n",
    "        raise ValueError(\"Either fixed_distance or fixed_number must be provided\")\n",
    "\n",
    "    dists = np.sqrt(np.sum(np.diff(points, axis=0) ** 2, axis=1))\n",
    "    cumulative = np.concatenate(([0], np.cumsum(dists)))\n",
    "\n",
    "    if fixed_distance is not None:\n",
    "        sample = np.arange(0, cumulative[-1], fixed_distance)\n",
    "    elif fixed_number is not None:\n",
    "        sample = np.linspace(0, cumulative[-1], fixed_number, endpoint=False)\n",
    "\n",
    "    return np.array([np.interp(sample, cumulative, points[:, i]) for i in range(points.shape[1])]).T\n",
    "\n",
    "# 計算道路邊界的法向量。\n",
    "# 法向量是垂直於道路邊界的向量，用於計算道路的寬度和方向。\n",
    "class Track:\n",
    "    def __init__(\n",
    "        self,\n",
    "        path_distance: np.ndarray,\n",
    "        path_nodes: np.ndarray,\n",
    "        path_width: np.ndarray,\n",
    "        interpolate: bool = True,\n",
    "        fixed_distance: float = 2.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path_distance (np.ndarray): distance between nodes with shape (n, 2)\n",
    "            path_nodes (np.ndarray): nodes with shape (n, 2, 3)\n",
    "            path_width (np.ndarray): width of the path with shape (n, 1)\n",
    "        \"\"\"\n",
    "        self.path_distance = np.float32(path_distance)\n",
    "        self.path_nodes = np.float32(path_nodes)\n",
    "        self.path_width = np.float32(path_width)\n",
    "\n",
    "        # slightly perturb for numerically stable normals\n",
    "        center = path_nodes[:, 0] + 1e-5 * np.random.randn(*path_nodes[:, 0].shape)\n",
    "        width = path_width\n",
    "\n",
    "        # compute left and right track using normal\n",
    "        d = np.diff(center, axis=0, append=center[:1])\n",
    "        n = np.stack([-d[:, 2], np.zeros_like(d[:, 0]), d[:, 0]], axis=1)\n",
    "        n = n / (np.linalg.norm(n, axis=1, keepdims=True) + 1e-5)\n",
    "\n",
    "        left = center + n * (width / 2)\n",
    "        right = center - n * (width / 2)\n",
    "\n",
    "        # loop around\n",
    "        center = np.concatenate([center, center])\n",
    "        left = np.concatenate([left, left])\n",
    "        right = np.concatenate([right, right])\n",
    "\n",
    "        # resample points so each point is fixed_distance apart\n",
    "        if interpolate:\n",
    "            center = interpolate_smooth(center, fixed_distance=fixed_distance)\n",
    "            left = interpolate_smooth(left, fixed_distance=fixed_distance)\n",
    "            right = interpolate_smooth(right, fixed_distance=fixed_distance)\n",
    "\n",
    "        # compute new cumulative distance (n,)\n",
    "        center_delta = np.diff(center, axis=0, prepend=center[:1])\n",
    "        center_delta_norm = np.linalg.norm(center_delta, axis=1)\n",
    "        self.center_distance = np.cumsum(center_delta_norm)\n",
    "\n",
    "        # (n, 3) points\n",
    "        self.center = center\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.width = interpolate_smooth(width, fixed_number=center.shape[0])\n",
    "\n",
    "    # 這個類別的主要作用是計算道路邊界的法向量，並且將其轉換為齊次座標。\n",
    "    # 這樣可以使模型能夠學習到道路的形狀和方向。\n",
    "    def get_boundaries(\n",
    "        self,\n",
    "        distance: float,\n",
    "        n_points: int = 10,\n",
    "        interpolate: bool = True,\n",
    "        fixed_distance: float = 2.5,\n",
    "    ) -> np.ndarray:\n",
    "        idx = np.searchsorted(self.center_distance, distance, side=\"left\")\n",
    "        center = self.center[idx : idx + n_points + 1]\n",
    "        width = self.width[idx : idx + n_points]\n",
    "\n",
    "        d = np.diff(center, axis=0)\n",
    "        n = np.stack([-d[:, 2], np.zeros_like(d[:, 0]), d[:, 0]], axis=1)\n",
    "        n = n / (np.linalg.norm(n, axis=1, keepdims=True) + 1e-7)\n",
    "        left = center[:-1] + n * (width / 2)\n",
    "        right = center[:-1] - n * (width / 2)\n",
    "\n",
    "        if interpolate:\n",
    "            center = interpolate_smooth(center, fixed_distance=fixed_distance)\n",
    "            left = interpolate_smooth(left, fixed_distance=fixed_distance)\n",
    "            right = interpolate_smooth(right, fixed_distance=fixed_distance)\n",
    "\n",
    "        left = homogeneous(left)\n",
    "        right = homogeneous(right)\n",
    "\n",
    "        return left, right\n",
    "\n",
    "    @cached_property\n",
    "    def track(self):\n",
    "        return homogeneous(self.center)\n",
    "\n",
    "    @cached_property\n",
    "    def track_left(self):\n",
    "        return homogeneous(self.left)\n",
    "\n",
    "    @cached_property\n",
    "    def track_right(self):\n",
    "        return homogeneous(self.right)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用來計算 L1 loss（絕對誤差），並細分為：\n",
    "  - 縱向誤差（longitudinal error）：沿著前進方向的誤差。\n",
    "  - 橫向誤差（lateral error）：左右偏移的誤差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class PlannerMetric:\n",
    "    \"\"\"\n",
    "    Computes longitudinal and lateral errors for a planner\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.l1_errors = []\n",
    "        self.total = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.l1_errors = []\n",
    "        self.total = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def add(\n",
    "        self,\n",
    "        preds: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        labels_mask: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            preds (torch.Tensor): (b, n, 2) float tensor with predicted waypoints\n",
    "            labels (torch.Tensor): (b, n, 2) ground truth waypoints\n",
    "            labels_mask (torch.Tensor): (b, n) bool mask for valid waypoints\n",
    "        \"\"\"\n",
    "        error = (preds - labels).abs()\n",
    "        error_masked = error * labels_mask[..., None]\n",
    "\n",
    "        # sum across batch and waypoints\n",
    "        error_sum = error_masked.sum(dim=(0, 1)).cpu().numpy()\n",
    "\n",
    "        self.l1_errors.append(error_sum)\n",
    "        self.total += labels_mask.sum().item()\n",
    "\n",
    "    def compute(self) -> dict[str, float]:\n",
    "        error = np.stack(self.l1_errors, axis=0)\n",
    "        longitudinal_error = error[:, 0].sum() / self.total\n",
    "        lateral_error = error[:, 1].sum() / self.total\n",
    "        l1_error = longitudinal_error + lateral_error\n",
    "\n",
    "        return {\n",
    "            \"l1_error\": float(l1_error),\n",
    "            \"longitudinal_error\": float(longitudinal_error),\n",
    "            \"lateral_error\": float(lateral_error),\n",
    "            \"num_samples\": self.total,\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
